{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "human_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqcf4l3NnbHnSuMIiKvRpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niyanchun/machine-vision-in-action/blob/master/autoencoder/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gd-W0JQDwb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "fc54f39a-982a-4405-e31e-0dac64c519aa"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow\n",
        "import cv2\n",
        "\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "\n",
        "# from pyimagesearch.conv_autoencoder import ConvAutoencoder\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose,\n",
        "                                     LeakyReLU, Activation, Flatten,\n",
        "                                     Dense, Reshape, Input)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class ConvAutoencoder:\n",
        "\n",
        "    @staticmethod\n",
        "    def build(width, height, depth, filters=(32, 64), latentDim=16):\n",
        "\n",
        "        inputShape = (height, width, depth)\n",
        "        chanDim = -1\n",
        "\n",
        "        # define the input to the encoder\n",
        "        inputs = Input(shape=inputShape)\n",
        "        x = inputs\n",
        "\n",
        "        for f in filters:\n",
        "            x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        # flatten the network and then construct our latent vector\n",
        "        volumeSize = K.int_shape(x)\n",
        "        x = Flatten()(x)\n",
        "        latent = Dense(latentDim)(x)\n",
        "\n",
        "        encoder = Model(inputs, latent, name=\"encoder\")\n",
        "        print(encoder.summary())\n",
        "\n",
        "        latentInputs = Input(shape=(latentDim,))\n",
        "        x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
        "        x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
        "\n",
        "        for f in filters[::-1]:\n",
        "            x = Conv2DTranspose(f, (3, 3), strides=2, padding=\"same\")(x)\n",
        "            x = LeakyReLU(alpha=0.2)(x)\n",
        "            x = BatchNormalization(axis=chanDim)(x)\n",
        "\n",
        "        x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
        "        outputs = Activation(\"sigmoid\")(x)\n",
        "\n",
        "        decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
        "\n",
        "        autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")\n",
        "\n",
        "        return encoder, decoder, autoencoder\n",
        "\n",
        "\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-s\", \"--samples\", type=int, default=8,\n",
        "#                 help=\"# number of samples to visualize when decoding\")\n",
        "# ap.add_argument(\"-o\", \"--output\", type=str, default=\"output.png\",\n",
        "#                 help=\"path to output visualization file\")\n",
        "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "#                 help=\"path to output plot file\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "args = {}\n",
        "args[\"samples\"] = 8\n",
        "args[\"output\"] = \"output.png\"\n",
        "args[\"plot\"] = \"plot.png\"\n",
        "\n",
        "Epoch = 25\n",
        "BatchSize = 32\n",
        "\n",
        "print(\"loading MNIST dataset...\")\n",
        "((trainX, _), (testX, _)) = mnist.load_data()\n",
        "\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "print(\"building autoencoder...\")\n",
        "(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n",
        "opt = Adam(lr=1e-3)\n",
        "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
        "\n",
        "H = autoencoder.fit(trainX, trainX, validation_data=(testX, testX), epochs=Epoch, batch_size=BatchSize)\n",
        "\n",
        "N = np.arange(0, Epoch)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "# loss: 训练集上的loss， val_loss: 测试集上的loss\n",
        "# loss一直下降、收敛，val_loss却上升、不收敛，说明过拟合了\n",
        "#\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "# plt.show()\n",
        "plt.savefig(args[\"plot\"])\n",
        "\n",
        "print(\"making predictions...\")\n",
        "decoded = autoencoder.predict(testX)\n",
        "outputs = None\n",
        "\n",
        "for i in range(0, args[\"samples\"]):\n",
        "    original = (testX[i] * 255).astype(\"uint8\")\n",
        "    recon = (decoded[i] * 255).astype(\"uint8\")\n",
        "\n",
        "    output = np.hstack([original, recon])\n",
        "    if outputs is None:\n",
        "        outputs = output\n",
        "\n",
        "    else:\n",
        "        outputs = np.vstack([outputs, output])\n",
        "\n",
        "cv2.imwrite(args[\"output\"], outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n",
            "loading MNIST dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "building autoencoder...\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                50192     \n",
            "=================================================================\n",
            "Total params: 69,392\n",
            "Trainable params: 69,200\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "60000/60000 [==============================] - 142s 2ms/sample - loss: 0.0183 - val_loss: 0.0110\n",
            "Epoch 2/25\n",
            " 9536/60000 [===>..........................] - ETA: 1:51 - loss: 0.0109"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}